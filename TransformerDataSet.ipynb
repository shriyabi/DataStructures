{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shriyabi/DataStructures/blob/main/TransformerDataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt8pls5xIWtT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "def extract_paragraphs(text, min_lines, n_paragraphs, line_split=\"\\n\"):\n",
        "    \"\"\"\n",
        "    Extracts paragraphs from a given text. A paragraph is defined as a contiguous\n",
        "    block of text separated by the specified line split character and having a minimum number of lines.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text from which paragraphs are to be extracted.\n",
        "        min_lines (int): The minimum number of lines required for a block of text to be considered a paragraph.\n",
        "        n_paragraphs (int): The number of paragraphs to extract that meet the minimum line requirement.\n",
        "        line_split (str, optional): The character or string used to split the text into lines. Default is '\\n'.\n",
        "\n",
        "    Returns:\n",
        "        list of str: A list containing the extracted paragraphs. Each paragraph is a string.\n",
        "\n",
        "    The function iterates through the lines of the input text, split by the line_split character.\n",
        "    It accumulates lines into a paragraph until a blank line or the line split character is encountered.\n",
        "    If the accumulated lines meet or exceed the `min_lines` requirement, the paragraph is added to\n",
        "    the result list. This process repeats until either the end of the text is reached or the required\n",
        "    number of paragraphs (`n_paragraphs`) is extracted. If the end of the text is reached and the current\n",
        "    paragraph meets the minimum line requirement but hasn't been added yet, it will be included in the result.\n",
        "    \"\"\"\n",
        "\n",
        "    lines = text.split(line_split)\n",
        "    paragraphs = []\n",
        "    current_paragraph = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip():  # Add non-empty lines to the current paragraph\n",
        "            current_paragraph.append(line)\n",
        "        else:\n",
        "            # Check if the current paragraph meets the minimum line requirement\n",
        "            if len(current_paragraph) >= min_lines:\n",
        "                paragraphs.append(line_split.join(current_paragraph))\n",
        "                if len(paragraphs) == n_paragraphs:  # Check if we have required number of paragraphs\n",
        "                    break\n",
        "            current_paragraph = []\n",
        "\n",
        "    # Check the last paragraph if end of text is reached\n",
        "    if current_paragraph and len(current_paragraph) >= min_lines and len(paragraphs) < n_paragraphs:\n",
        "        paragraphs.append(line_split.join(current_paragraph))\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "def wc(x):\n",
        "\treturn len(x.split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Short Stories\n",
        "file_names = ['stories.csv']\n",
        "list_of_snippets = []\n",
        "df = pd.read_csv(file_name);\n",
        "contents = df.iloc[:,-1].to_dict()\n",
        "for i in range(100):\n",
        "  content = extract_paragraphs_on_content(contents.get(i))\n",
        "  list_of_snippets.append({f'short_stories_{i}': content})\n",
        "json_result = json.dumps(list_of_snippets)\n",
        "\n",
        "print(json_result)"
      ],
      "metadata": {
        "id": "yYV8UOMTIdhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Songs\n",
        "#df = pd.read_csv(wasabi_songs.csv)\n",
        "#songs = dcterms:title\n",
        "#chords = dcterms:chord #am i doing this correctly and where is verse\n",
        "#list_of_snippets = {}\n",
        "#for i in range(100):\n",
        "  #song_name = songs.get(i)\n",
        "  #content = chords.get(i)\n",
        "  #list_of_snippets.append({str('song_' + i):f'{song_name}','snippet': f'{content}'})\n",
        "#json_result = json.dumps(list_of_snippets)"
      ],
      "metadata": {
        "id": "h5Vg5TsbIeld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#News Articles\n",
        "file_names = [file for file in os.listdir(/Users/sbiddala/downloads/bbc/politics) if file.endswith('.txt')]\n",
        "list_of_snippets = []\n",
        "limit = 100\n",
        "df = pd.read_csv(file_name);\n",
        "for i,file_name in file_names:\n",
        "  if i >= limit:\n",
        "    break\n",
        "  file_path = os.path.join(/Users/sbiddala/downloads/bbc/politics,file_name)\n",
        "  with open(file_path,'r',encoding='utf-8') as file:\n",
        "    contents = file.read()\n",
        "    content = extract_paragraphs_on_content(contents)\n",
        "    list_of_snippets.append({f'new_articles_{i}': content})\n",
        "json_result = json.dumps(list_of_snippets)\n",
        "\n",
        "print(json_result)"
      ],
      "metadata": {
        "id": "yEyfxfLyIe0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Movie Scripts\n",
        "#df = pd.read_csv(imdb_top_1000.csv)\n",
        "#contents = df.iloc[:,8].to_dict()\n",
        "#movie_names = df.iloc[:,2].to_dict()\n",
        "#list_of_snippets = {}\n",
        "#for i in range(100):\n",
        "  #movie_name = movie_names.get(i)\n",
        "  #content = contents.get(i)\n",
        "  #val = extract_paragraphs_on_content(content)\n",
        "  #list_of_snippets.append({str('article_' + i):f'{movie_name}','snippet': f'{val}'})\n",
        "#json_result = json.dumps(list_of_snippets)"
      ],
      "metadata": {
        "id": "cCuqDTlGIfCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Op-eds\n",
        "file_names = ['MultiOpEd.csv']\n",
        "list_of_snippets = []\n",
        "df = pd.read_csv(file_name);\n",
        "for i in range(100):\n",
        "  df['wc'] = df['articles'].apply(lambda x: wc(x))\n",
        "  25_per = np.percentile(df['word_count'], 25)\n",
        "  50_per = np.percentile(df['word_count'], 50)\n",
        "  valid_articles = df.query(\"wc >= {25_per} and wc <= {50_per}\")\n",
        "  sample = valid_articles.sample(100, random_state=42)\n",
        "  list_of_snippets.append({f'short_stories_{i}': sample})\n",
        "json_result = json.dumps(list_of_snippets)\n",
        "\n",
        "print(json_result)"
      ],
      "metadata": {
        "id": "fZU3phmRxc4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}